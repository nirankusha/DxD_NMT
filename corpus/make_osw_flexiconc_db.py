# -*- coding: utf-8 -*-
"""make_osw_flexiconc_db.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UBblxdHgkP5hgScImgqelN9wlRCVrAO0
"""

#!/usr/bin/env python3
"""
Build FlexiConc DB from OSW export and patch spans_file with metadata.

Steps:
1) Load /export/pl and /export/en into FlexiConc via TextImport.
2) Find the created SQLite database file.
3) Ensure spans_file has extra columns and patch from file_metadata.csv.

Usage:
  python make_osw_flexiconc_db.py --export-root /path/to/exports --db-name osw.sqlite
"""

import argparse
import sqlite3
from pathlib import Path
from typing import Optional, List
import time
import pandas as pd

# FlexiConc
from flexiconc import TextImport

def load_into_flexiconc(export_root: Path, db_name: str) -> Path:
    """
    Load PL/EN docs into FlexiConc.
    Try to get a DB path; if the API doesn't expose one, we search heuristically.
    """
    pl_dir = export_root / "pl"
    en_dir = export_root / "en"
    if not pl_dir.exists() and not en_dir.exists():
        raise SystemExit(f"No 'pl' or 'en' directories under: {export_root}")

    ti = TextImport()
    paths: List[str] = []
    if pl_dir.exists():
        paths.append(str(pl_dir))
    if en_dir.exists():
        paths.append(str(en_dir))

    print(f"üì• Loading {len(paths)} folders into FlexiConc...")
    ti.load_files(paths=paths, use_spacy=False)

    # Try to discover DB path automatically
    # Strategy A: attribute
    db_path: Optional[Path] = None
    cand_attrs = ("db_path", "database_path", "dbfile", "db_file")
    for attr in cand_attrs:
        if hasattr(ti, attr):
            v = getattr(ti, attr)
            if v:
                db_path = Path(v)
                break

    # Strategy B: search for recent .sqlite files under export_root (and its parent)
    def newest_sqlite(base: Path) -> Optional[Path]:
        all_sqlites = list(base.rglob("*.sqlite")) + list(base.rglob("*.db"))
        if not all_sqlites:
            return None
        all_sqlites.sort(key=lambda p: p.stat().st_mtime, reverse=True)
        return all_sqlites[0]

    if db_path is None:
        # give the system a moment to flush files
        time.sleep(0.5)
        db_path = newest_sqlite(export_root)
        if db_path is None:
            db_path = newest_sqlite(Path.cwd())

    if db_path is None:
        print("‚ö†Ô∏è  Could not auto-detect a DB file created by TextImport.")
        # As a fallback, create an empty DB name under export_root and let the user patch manually
        db_path = export_root / db_name
        print(f"Creating a new DB stub at: {db_path}")
        sqlite3.connect(str(db_path)).close()

    print(f"‚úÖ Using database: {db_path}")
    return db_path


def ensure_columns(conn: sqlite3.Connection, table: str, columns: dict):
    """Ensure columns exist on table; columns is {name: SQLTYPE}."""
    cur = conn.cursor()
    cur.execute(f"PRAGMA table_info({table})")
    existing = {row[1] for row in cur.fetchall()}
    for col, typ in columns.items():
        if col not in existing:
            cur.execute(f"ALTER TABLE {table} ADD COLUMN {col} {typ}")
    conn.commit()


def detect_filename_column(conn: sqlite3.Connection, table: str) -> Optional[str]:
    """Guess which column stores filename/path in spans_file."""
    cur = conn.cursor()
    cur.execute(f"PRAGMA table_info({table})")
    cols = [r[1] for r in cur.fetchall()]
    for candidate in ("filename", "file", "path", "name"):
        if candidate in cols:
            return candidate
    return None


def patch_spans_file_with_metadata(db_path: Path, meta_csv: Path):
    if not meta_csv.exists():
        raise SystemExit(
            f"Missing metadata CSV: {meta_csv}\n"
            "Run pelcra_parser.py first with the correct --xml and --out."
        )

    df = pd.read_csv(meta_csv)
    if "filepath" not in df.columns:
        raise SystemExit("file_metadata.csv must contain a 'filepath' column.")

    # We match by basename
    df["__key"] = df["filepath"].apply(lambda p: Path(p).name)

    conn = sqlite3.connect(str(db_path))
    try:
        # ensure extra columns on spans_file
        ensure_columns(conn, "spans_file", {
            "text_id":        "TEXT",
            "lang":           "TEXT",
            "bibl_id":        "TEXT",
            "title_a":        "TEXT",
            "title_j":        "TEXT",
            "author":         "TEXT",
            "date_published": "TEXT",
            "url":            "TEXT",
        })

        fname_col = detect_filename_column(conn, "spans_file")
        if not fname_col:
            raise SystemExit(
                "Could not find a filename/path column on spans_file. "
                "Inspect the DB schema (PRAGMA table_info(spans_file)) and adjust the script."
            )

        cur = conn.cursor()
        cur.execute(f"SELECT id, {fname_col} FROM spans_file")
        rows = cur.fetchall()
        if not rows:
            print("‚ö†Ô∏è  No rows in spans_file yet; did TextImport create file spans?")
        updated = 0
        for span_id, filename in rows:
            if not filename:
                continue
            key = Path(filename).name
            hit = df.loc[df["__key"] == key]
            if hit.empty:
                continue
            r = hit.iloc[0]
            cur.execute("""
                UPDATE spans_file
                   SET text_id=?,
                       lang=?,
                       bibl_id=?,
                       title_a=?,
                       title_j=?,
                       author=?,
                       date_published=?,
                       url=?
                 WHERE id=?""",
                (
                    r.get("text_id"), r.get("lang"), r.get("bibl_id"),
                    r.get("title_a"), r.get("title_j"), r.get("author"),
                    r.get("date_published"), r.get("url"),
                    span_id
                )
            )
            updated += 1
        conn.commit()
        print(f"‚úÖ Patched spans_file rows: {updated}")
    finally:
        conn.close()


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--export-root", required=True, help="Directory produced by pelcra_parser.py")
    ap.add_argument("--db-name", default="osw.sqlite", help="Optional DB filename (used if auto-detection fails)")
    args = ap.parse_args()

    export_root = Path(args.export_root)
    meta_csv = export_root / "file_metadata.csv"
    if not export_root.exists():
        raise SystemExit(f"Export root not found: {export_root}")

    # 1) Load into FlexiConc and find DB
    db_path = load_into_flexiconc(export_root, args.db_name)

    # 2) Patch spans_file with metadata
    patch_spans_file_with_metadata(db_path, meta_csv)

    print("\nüéâ Done. You can now query with FlexiConc and see file-level metadata on results.")

if __name__ == "__main__":
    main()